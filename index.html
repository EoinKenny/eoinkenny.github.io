<!doctype html>
<html>
  <head>
    <!-- Page setup -->
    <meta charset="utf-8">
    <title>Portfolio Website</title>
    <meta name="description" content="Eoin Kenny: Researcher Personal Website">
    <meta name="author" content="Postoctoral Associate at MIT">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
    <link rel="icon" type="image/png" href="favicon.png">
  
    <!-- Stylesheets -->
    <!-- Reset default styles and add support for google fonts -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" rel="stylesheet" type="text/css" />
    <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css" />
   
    <!-- Custom styles -->
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>    

    <!-- Want to add Bootstrap? -->
    <!-- Visit: https://getbootstrap.com/docs/4.3/getting-started/introduction/ -->
    
  </head>
  
  <body>

    <header id="header">
      <img src="logo.jpg", width="200">
      <h1>Eoin M. Kenny - Postdoc at MIT</h1>
      
      <!-- Menu link fragment #id should match a div id. Example: <a href="#home"> links to <div id="home"></div>  -->
      <ul class="main-menu">
        <li><a href="#home">Home</a></li>
        <li><a href="#research">Research</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>                 
    </header>
   
    <div id="container">
      <div class="inner">
        <div id="content"> 
          
          <div id="home" class="content-region hide">
            <h2>Home</h2>

            <p>
              I am an explainable AI reseracher. Previously, I did my Ph.D. at University College Dublin, Ireland. For my undergrad, I completed my Bachelor of Music degree (and Master of Arts in Musicology & Performance) at the University of Maynooth. Currently, I am at MIT primarily researching interpretable deep reinforcement learning for my postdoc.
            </p>

            <p>
              My research vision is to design interpretable AI systems which reason in the same way humans do, so that we can clearly see what they are doing in a causally faithful way that everyone can understand (not just ML experts). This will help us successfully utilize their abilities in sensitive domains such as medicine, finance, and autonomous vechicles where people with various backgrounds need to interact with the systems.
            </p>

            <p>
              Specifically, I believe in using exemplar/prototype theory, contrative explanation, and causal reasoning (either in the sense of the world or model) in this process. My strongest contributions to the field have been (1) the introduction of Semi-Factual explanation, and (2) designing the first inherently interpretable Deep RL system.
            </p>

            <p>
              Above all however, I am a strong advocate for testing these systems on real humans to prove their utility.

            <p>
              Feel free to reach out if you would like to talk.
            </p>

          </div>
          
          <div id="research" class="content-region hide">
            <p>






            <!-- Start One paper-->
            <h3>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/iclr2023.png" width="400">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We build the first inherently interpretable, general, well performaning, deep reinforcement learning algorithm.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an "interpretable-by-design" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.
                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Mycal Tucker, Julie A. Shah</i>
          </p>
          <p>
                [<a href="https://openreview.net/forum?id=hWwY_Jq0xsN">ICLR 2023</a>] * Spotlight Presentation (top 25% of accepted papers)
          </p>
        </div>

        <hr>
            <!-- End One paper-->



            <!-- Start One paper-->
            <h3>Chapter 13 - User tests & techniques for the post-hoc explanation of deep learning</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/explainableDeepAI.jpg" width="400">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We contexualise post-hoc explanation and show a novel method to generate semi-factual and counterfactual images.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> This chapter elaborates on some key ideas and studies designed to provide post-hoc explanations-by-example to the problem of explaining the predictions of black-box deep-learning systems (the so-called XAI problem). With a focus on image and time series data we review several recent explanation strategies – using factual, counterfactual, and semifactual explanations – for Deep Learners. Several novel evaluations of these methods are reported showing how well these methods work, along with representative outputs that are produced. The chapter also profiles the user studies being carried out on these methods, discussing the pitfalls that arise and results found.


                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin Delaney, Eoin M. Kenny, Derek Greene, Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/B9780323960984000193?via%3Dihub">Explainable Deep Learning AI 2023</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper --> 










            <!-- Start One paper-->
            <h3>Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XAI</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/kbs 2022.jpg" width="400">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show how the twin-system framework can be used to explain tabular, image, and text data predictions with post-hoc explanation by example..

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In this paper, the twin-systems approach is reviewed, implemented, and competitively tested as a post-hoc explanation-by-example solution to the eXplainable Artificial Intelligence (XAI) problem. In twin-systems, an opaque artificial neural network (ANN) is explained by “twinning” it with a more interpretable case-based reasoning (CBR) system, by mapping the feature weights from the former to the latter. Extensive comparative tests are performed, over four experiments, to determine the optimal feature-weighting method for such twin-systems. Twin-systems for traditional multilayer perceptron (MLP) networks (MLP–CBR twins), convolutional neural networks (CNNs; CNN–CBR twins), and transformers for NLP (BERT–CBR twins) are examined. In addition, Feature Activation Maps (FAMs) are explored to enhance explainability by providing an additional layer of explanatory insight. The wider implications of this …



                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin Delaney, Eoin M. Kenny, Derek Greene, Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/S0950705121007929">Knowledge Based Systems 2022</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->






            </p>
          </div>
          
          <div id="contact" class="content-region hide">
            <h2>Contact</h2>
            <p>
              email: ekenny (at...) mit (dot...) edu
            </p>
            <p>
              <a href="https://www.linkedin.com/in/eoin-kenny-92b57b22/">LinkedIn</a>
            </p>
            <p>
              <a href="Twitter: https://twitter.com/EoinKNNy/">Twitter</a>
            </p>
          </div>


          
        </div>
      </div>
    </div>

<!--     <footer>  
      Footer region
    </footer>
 -->    
    <!-- Load additional JS scripts here -->
    <script type="text/javascript" src="script.js"></script>
    
  </body>
</html>
