<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Eoin M. Kenny</title>
  <meta name="description" content="Eoin Kenny's homepage">

  <link href="./css/github-light.css" rel="stylesheet">
  <link href="./css/style.css" rel="stylesheet">
</head>
<body>
  <div class="container">
    <header>
      <h1>Eoin M. Kenny</h1>
      <img src="./assets/me.jpg" style="width:190px;"><br />
      <p>
        eoin.kenny (at) jpmorgan (dot) com<br />
        Senior Associate AI Researcher<br />
	<a href="https://www.jpmorgan.com/technology/artificial-intelligence/initiatives/explainable-ai-center-of-excellence/">Trustworthy AI</a>, J.P. Morgan<br />
	<a href= "https://x.com/EoinKNNy">X</a><br />
	<a href="https://scholar.google.com/citations?user=AzMTFY4AAAAJ&hl=en">Google Scholar</a>

      </p>
    </header>

    <section class="content">
      <h3>About</h3>
      <p>
		  
		  Hello, I am a Senior AI researcher at J.P. Morgan Chase, London, in their Trustworthy AI research group. Prior to this, I did my Ph.D. at University College Dublin with Mark Keane, and Postdoc at MIT with Julie Shah.
	      My vision for Explainable AI is that we must transition from developing XAI systems without clear objectives, to rigorously prioritising purpose-driven XAI that delivers true clarity and indispensable utility to the practitioners it serves.
      </p>

      <p>
        My contributions which I am most proud of are in interpretable  reinforcment learning with self-driving cars, and contrastive explanation in recourse.
      </p>

      <p>
        In my future research, I plan to focus on developing explainable systems which focus on purpose-driven evaluation, and have several papers in the works moving towards this vision.
      </p>


  



      <h3>Work Experience</h3>
      <ul>
        <li>J.P. Morgan, Senior Associate AI Researcher, 2024 (onwards)</li>
        <li>Postdoctoral Associate, MIT, 2022-2024</li>
        <li>Internship, Motional, 2023-2024</li>
      </ul>



      <h3>Education</h3>
      <ul>
        <li>Ph.D. in Computer Science, UCD, 2022</li>
        <li>M.S. in Computer Science, UCD, 2019</li>
        <li>M.A in Musicology & Performance, Maynooth University, 2013</li>
        <li>BMus, Maynooth University, 2010</li>
      </ul>













      <h3>Selected Publications (from <a href="https://scholar.google.com/citations?user=AzMTFY4AAAAJ&hl=en">Google Scholar</a>)</h3>

      <!-- p>
        These selected papers best represent my current research vision, currently I am working on the deployment of interpretable self-driving cars for error debugging, and transparent LLMs for human-AI collaboration.
      </p>
      <p>
        In the future, I am also interested in further developing semifactual explanation. Counterfactuals and causality have been shown to be crucial to explanation, but I believe semifactuals will play a huge role in the future too.
      </p>

      <hr> -->




            <!-- Start One paper-->
            <h4>Explainable deep learning improves human mental models of self-driving cars</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/nature.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show that concept-bottlneck models are useful in self-driving cars during deployment to help humans better understand the model's decision making process.

                        </p>

                      </div>
                        <!-- <p> -->

          <!--   <b>Abstract:</b> When users receive either a positive or negative outcome from an automated system,
 eXplainable AI (XAI) has almost exclusively focused on how to mutate negative
 outcomes into positive ones by crossing a decision boundary using counterfactuals
 (e.g., “If you earn 2k more, we will accept your loan application”). In this work,
 we instead focus on positive outcomes, and take the novel step of using XAI to
 optimize them (e.g., “Even if you wish to half your down-payment, we will still
 accept your loan application”). Explanations such as these that employ “even if”
 reasoning and do not cross a decision boundary, are known as semi-factuals. To
 instantiate semi-factuals in this context, we introduce the concept of “gain” (i.e.,
 how much a user stands to benefit from the proposed explanation), and consider
 the first causal formalization of semi-factuals. Tests on benchmark datasets show
 that our algorithms are better at maximizing gain compared to prior work, and
 that causality is especially important in the process. Most importantly however,
 a user study supports our main hypothesis by showing that people clearly find
 semi-factual explanations more useful compared to counterfactuals when they
 receive the positive outcome of a loan acceptance
                        </p> -->

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Akshay Dharmavaram, Sang Uk Lee, Tung Phan-Minh, Shreyas Rajesh, Yunqing Hu, Laura Major, Momchil S Tomov, Julie A. Shah</i>
          </p>
          <p>
                [<a href="https://www.researchsquare.com/article/rs-5515263/v1">Under First Round Revision at Nature</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->






            <!-- Start One paper-->
            <h4>The Utility of "Even if" Semifactual Explanation to Optimize Positive Outcomes</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/nips2023.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show that semifactuals are more useful than conterfactuals when a user gets a positive outcome from an AI system.

                        </p>

                      </div>
                        <!-- <p> -->

          <!--   <b>Abstract:</b> When users receive either a positive or negative outcome from an automated system,
 eXplainable AI (XAI) has almost exclusively focused on how to mutate negative
 outcomes into positive ones by crossing a decision boundary using counterfactuals
 (e.g., “If you earn 2k more, we will accept your loan application”). In this work,
 we instead focus on positive outcomes, and take the novel step of using XAI to
 optimize them (e.g., “Even if you wish to half your down-payment, we will still
 accept your loan application”). Explanations such as these that employ “even if”
 reasoning and do not cross a decision boundary, are known as semi-factuals. To
 instantiate semi-factuals in this context, we introduce the concept of “gain” (i.e.,
 how much a user stands to benefit from the proposed explanation), and consider
 the first causal formalization of semi-factuals. Tests on benchmark datasets show
 that our algorithms are better at maximizing gain compared to prior work, and
 that causality is especially important in the process. Most importantly however,
 a user study supports our main hypothesis by showing that people clearly find
 semi-factual explanations more useful compared to counterfactuals when they
 receive the positive outcome of a loan acceptance
                        </p> -->

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Weipeng Huang</i>
          </p>
          <p>
                [<a href="https://neurips.cc/virtual/2023/poster/71676">NeurIPS 2023</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->














            <!-- Start One paper
            <h4>Adv ancing Post-Hoc Case-Based Explanation with Feature Highlighting</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/ijcai2023.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show how to add feature saliency to post-hoc case-based explanation.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> Explainable AI (XAI) has been proposed as a valuable tool to assist in downstream tasks involving
human-AI collaboration. Perhaps the most psychologically valid XAI techniques are case-based approaches which display “whole” exemplars to
explain the predictions of black-box AI systems.
However, for such post-hoc XAI methods dealing
with images, there has been no attempt to improve
their scope by using multiple clear feature “parts”
of the images to explain the predictions while linking back to relevant cases in the training data, thus
allowing for more comprehensive explanations that
are faithful to the underlying model. Here, we
address this gap by proposing two general algorithms (latent and superpixel-based) which can iso-
late multiple clear feature parts in a test image, and
then connect them to the explanatory cases found
in the training data, before testing their effectiveness in a carefully designed user study. Results
demonstrate that the proposed approach appropriately calibrates a user’s feelings of “correctness”
for ambiguous classifications in real world data on
the ImageNet dataset, an effect which does not happen when just showing the explanation without feature highlighting
                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Eoin Delaney, and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2023/0048.pdf">IJCAI 2023</a>]
          </p>
        </div>

        <hr> -->
            <!-- End One paper-->





            <!-- Start One paper-->
            <h4>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/iclr2023.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We build the first inherently interpretable, general, well performaning, deep reinforcement learning algorithm.

                        </p>

                      </div >
                        <!-- <p>

            <b>Abstract:</b> Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an "interpretable-by-design" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.
                        </p -->

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Mycal Tucker, and Julie A. Shah</i>
          </p>
          <p>
                [<a href="https://openreview.net/forum?id=hWwY_Jq0xsN">ICLR 2023</a>] * <b>Spotlight Presentation</b> (top 25% of accepted papers)
          </p>
        </div>

        <hr>
            <!-- End One paper-->



            <!-- Start One paper-->
            <!--  <h3>Chapter 13 -  User tests & techniques for the post-hoc explanation of deep learning</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/explainableDeepAI.jpg" width="400">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We contexualise post-hoc explanation and show a novel method to generate semi-factual and counterfactual images.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> This chapter elaborates on some key ideas and studies designed to provide post-hoc explanations-by-example to the problem of explaining the predictions of black-box deep-learning systems (the so-called XAI problem). With a focus on image and time series data we review several recent explanation strategies – using factual, counterfactual, and semifactual explanations – for Deep Learners. Several novel evaluations of these methods are reported showing how well these methods work, along with representative outputs that are produced. The chapter also profiles the user studies being carried out on these methods, discussing the pitfalls that arise and results found.


                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin Delaney, Eoin M. Kenny, Derek Greene and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/B9780323960984000193?via%3Dihub">Explainable Deep Learning AI 2023</a>]
          </p>
        </div>

        <hr> -->
            <!-- End One paper --> 










            <!-- Start One paper-->
           <!--  <h4>Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XAI</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/kbs2022.jpg" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We advance our state-of-the-art twin-system framework and show how to explain tabular, image, and text data AI predictions with training examples.
                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In this paper, the twin-systems approach is reviewed, implemented, and competitively tested as a post-hoc explanation-by-example solution to the eXplainable Artificial Intelligence (XAI) problem. In twin-systems, an opaque artificial neural network (ANN) is explained by “twinning” it with a more interpretable case-based reasoning (CBR) system, by mapping the feature weights from the former to the latter. Extensive comparative tests are performed, over four experiments, to determine the optimal feature-weighting method for such twin-systems. Twin-systems for traditional multilayer perceptron (MLP) networks (MLP–CBR twins), convolutional neural networks (CNNs; CNN–CBR twins), and transformers for NLP (BERT–CBR twins) are examined. In addition, Feature Activation Maps (FAMs) are explored to enhance explainability by providing an additional layer of explanatory insight. The wider implications of this …



                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/S0950705121007929">Knowledge Based Systems 2022</a>]
          </p>
        </div>

        <hr>
         End One paper-->






            <!-- Start One paper-->
            <h4>On generating plausible counterfactual and semi-factual explanations for deep learning</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/aaai2022.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We introduce the AI world to semi-factuals, and show a plausible way to generate them (and counterfactuals) using a framework called PIECE.

                        </p>

                      </div>
<!--                         <p>

            <b>Abstract:</b> There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet, counterfactual explanations have become very popular in eXplainable AI (XAI) due to their asserted computational, psychological, and legal benefits. In contrast however, semi-factuals (which appear to be equally useful) have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly because the non-discrete nature of images makes good counterfactuals difficult to define; indeed, generating plausible counterfactual images which lie on the data manifold is also problematic. This paper advances a novel method for generating plausible counterfactuals and semi-factuals for black-box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all “exceptional” features in a test image to be “normal” from the perspective of the counterfactual class, to generate plausible counterfactual images. Two controlled experiments compare this method to others in the literature, showing that PIECE generates highly plausible counterfactuals (and the best semi-factuals) on several benchmark measures.




                        </p> -->

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17377">AAAI 2022</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->




            <!-- Start One paper-->
            <h4>Bayesian Case-Exclusion and Explainable AI (XAI) for Sustainable Farming</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/iccbr2019.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show how to accuractly predict grass growth and offer "good" explanations to Irish Dairy Farmers.

                        </p>

                      </div>
<!--                         <p>

            <b>Abstract:</b> Smart agriculture (SmartAg) has emerged as a rich domain for AI-driven decision support systems (DSS); however, it is often challenged by user-adoption issues. This paper reports a case-based reasoning system, PBI-CBR, that predicts grass growth for dairy farmers, that combines predictive accuracy and explanations to improve user adoption. PBI-CBR’s key novelty is its use of Bayesian methods for case-base maintenance in a regression domain. Experiments report the tradeoff between predictive accuracy and explanatory capability for different variants of PBI-CBR, and how updating Bayesian priors each year improves performance.





                        </p>
 -->
        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M Kenny, Elodie Ruelle, Anne Geoghegan, Mohammed Temraz, Mark T Keane</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2020/657">IJCAI 2020</a>] <i>Sister Conference Best Paper Track</i> * <b>Best Paper Award</b> at ICCBR 2019.
          </p>
        </div>

        <hr>
            <!-- End One paper-->



            <!-- Start One paper-->
            <h4>Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/aij2021.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We find that nearest neighbor exemplar-based explanation lead people to view classifiction errors as being “less incorrect”, moreover they do not effect trust.

                        </p>

                      </div>
<!--                         <p>

            <b>Abstract:</b> In this paper, we describe a post-hoc explanation-by-example approach to eXplainable AI (XAI), where a black-box, deep learning system is explained by reference to a more transparent, proxy model (in this situation a case-based reasoner), based on a feature-weighting analysis of the former that is used to find explanatory cases from the latter (as one instance of the so-called Twin Systems approach). A novel method (COLE-HP) for extracting the feature-weights from black-box models is demonstrated for a convolutional neural network (CNN) applied to the MNIST dataset; in which extracted feature-weights are used to find explanatory, nearest-neighbours for test instances. Three user studies are reported examining people's judgements of right and wrong classifications made by this XAI twin-system, in the presence/absence of explanations-by-example and different error-rates (from 3-60%). The judgements gathered include item-level evaluations of both correctness and reasonableness, and system-level evaluations of trust, satisfaction, correctness, and reasonableness. Several proposals are made about the user's mental model in these tasks and how it is impacted by explanations at an item- and system-level. The wider lessons from this work for XAI and its user studies are reviewed.



                        </p> -->

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M Kenny, Courtney Ford, Molly Quinn, Mark T Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/S0004370221000102">Artificial Intelligence 2021</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->












            <!-- Start One paper
            <h4>If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual xai techniques</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/ijcai2021.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We highlight the shortcomings in counterfactual XAI research evaluation, and suggest solutions.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.






                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Mark T Keane, Eoin M Kenny, Eoin Delaney and Barry Smyth</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2021/0609.pdf">IJCAI 2021</a>]
          </p>
        </div>

        <hr>
           End One paper-->








            <!-- Start One paper
            <h4>Generating Plausible Counterfactual Explanations for
Deep Transformers in Financial Text Classification</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/coling2020.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show how to generate counterfactual explanations for NLP in a model agnostic way.

                        </p>

                      </div>
                         <p>

            <b>Abstract:</b> Corporate mergers and acquisitions (M&A) account for billions of dollars of investment globally every year, and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust and accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.






                        </p> 

        </div>

        <div class='paper-contents'>

          <p>
            <i>Linyi Yang, Eoin M Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, Ruihai Dong</i>
          </p>
          <p>
                [<a href="https://aclanthology.org/2020.coling-main.541/">COLING 2020</a>]
          </p>
        </div>

        <hr>
             End One paper-->








            <!-- Start One paper
            <h4>Twin-systems to explain artificial neural networks using case-based reasoning: Comparative tests of feature-weighting methods in ANN-CBR twins for XAI</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/ijcai2019.jpg" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We propose a new framework for post-hoc explanation-by-example called Twin-Systems, and show it is much better compared to prior work.

                        </p>

                      </div>
                         <p>

            <b>Abstract:</b> In this paper, twin-systems are described to address the eXplainable artificial intelligence (XAI) problem, where a black box model is mapped to a white box “twin” that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artificial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to find cases that explain the ANN’s outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the ”twinning” of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN’s classifications.





                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2019/376">IJCAI 2019</a>]
          </p>
        </div>

        <hr>
            End One paper-->








      <h3>Media</h3>
            <!-- Start One paper-->
            <h4>National AI Awards Ireland</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/aiAwardIreland.jpeg" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> I won the best application of AI in a student project for my work in Explainable AI in Smart Agriculture.

                        </p>

                      </div>
                        <p>

<!--             Awarded to Eoin Kenny- A PhD student in University College Dublin for his internationally recognised work on predictive AI modelling, explainable AI and precision agriculture.
 -->






                        </p>

        </div>

        <div class='paper-contents'>


          <p>
                [<a href="https://www.agriland.ie/farming-news/grass-growth-prediction-research-secures-top-ai-award/">Agriland Press Release</a>]
          </p>

        </div>

        <hr>
            <!-- End One paper-->








            <!-- Start One paper-->
            <h4>International Conference on Case-Based Reasoning</h4>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/bestPaper.jpeg" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> I won the best paper award at ICCBR 2019 which was themed on Explainable AI for my paper regarding Smart Agriculture.

                        </p>

                      </div>
                        <p>

<!--             Eoin Kenny, a Ph.D. candidate in computer science at the Insight Centre for Data Analytics, recently received an award for best paper at the International Conference on Case-Based Reasoning (2019). 

The work emanated from the VistaMilk research project involving Insight, Teagasc, Tyndall, TSSG and ICBF. The main aim of this initiative is to promote smart agriculture in Ireland which both increases profits and improves sustainability of farming practices. We used an artificial intelligence​ technique called case-based reasoning (CBR) to augment an existing decision support application called PastureBase Ireland (PBI) used by Irish dairy farmers. The system predicts grass growth on Irish dairy farms in advance which allows greater utilisation of grass as a food source for cattle and lessens the carbon costs of importing meal. Continual meetings over several months between Insight and Teagasc were a crucial part of the research that allowed us access to the necessary materials and domain knowledge to design a workable system.
 -->                        </p>

        </div>




      <h3>Invited Talks (not exhaustive)</h3>

        <ul>
          <li>Jun 2023, ML Labs at UCD: One way to do a Ph.D. (and postdoc) </li>
          <li>Dec 2022, Navy Centre for Applied Research in Artificial Intelligence: Interpretable Deep Reinforcement Learning</li>
          <li>Mar 2022, Imperial College London: Explaining Black-box algorithms</li>
          <li>Feb 2022, Robert Gordon University: On the utility of explanation-by-example</li>
          <li>Nov 2019, KBC Data Science Bootcamp: Explaining Artificial Intelligence Black-Box Systems</li>
        </ul>







      <h3>Academic Services</h3>
      <ul>
	      <li>Reviewer, NeurIPS 2021-2023</li>
	      <li>Reviewer, ICLR 2023</li>
	      <li>Reviewer, AAAI 2021</li>
	      <li>Reviewer, ICML 2022-2023</li>
        <li>Reviewer, <i>Artificial Intelligence</i> Journal 2021-2022</li>
      </ul>








    </section>
  </div>
</body>
</html>
