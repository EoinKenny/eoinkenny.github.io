<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Eoin M. Kenny</title>
  <meta name="description" content="Eoin Kenny's homepage">

  <link href="./css/github-light.css" rel="stylesheet">
  <link href="./css/style.css" rel="stylesheet">
</head>
<body>
  <div class="container">
    <header>
      <h1>Alice Oh</h1>
      <img src="./assets/me.jpg" style="width:190px;"><br />
      <p>
        eoinkenny (at) mit.edu<br />
        Postdoctoral Associate<br />
	<a href="https://www.csail.mit.edu/">CSAIL</a>, MIT<br />
<!-- 	Director, <a href= "http://ml.kaist.ac.kr">MARS AI Research Center</a><br />
 -->	<a href="https://scholar.google.com/citations?user=AzMTFY4AAAAJ&hl=en">Google Scholar</a>

      </p>
    </header>

    <section class="content">
      <h3>About</h3>
      <p>
		  
		  Hello, I am a postdoctoral associate at MIT in CSAIL with joint appointment at AeroAstro.
	      My research interest is in the safe deployment of machine learning systems with Explainable AI (XAI). 
      </p>

      <p>
        <b>I envision AI systems which can be successfully deployed with useful, human-friendly explanations</b>, so that we can clearly see what they are doing in a way that everyone can understand (not just ML experts). To do this, I use <i>example-based</i> XAI, because it is similar to how humans are thought to reason and has much support in user testing that it is useful and understandable to people.
      </p>

      <p>
        My strongest contributions to the field have been (1) the introduction of Semi-Factual explanation, and (2) designing the first exemplar-based interpretable Deep RL system.
      </p>










      <h3>Education</h3>
      <ul>
        <li>Ph.D. in Computer Science, UCD, 2022</li>
        <li>M.S. in Computer Science, UCD, 2019</li>
        <li>M.A in Musicology & Performance, Maynooth University, 2013</li>
        <li>BMus, Maynooth University, 2010</li>
      </ul>













      <h3>Recent Publications (<a href="https://scholar.google.co.kr/citations?user=B88-xMEAAAAJ&hl=en">Google Scholar</a>)</h3>





            <!-- Start One paper-->
            <h3>If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual xai techniques</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/ijcai2021.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We highlight the shortcomings in counterfactual XAI research evaluation, and suggest solutions.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.






                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Mark T Keane, Eoin M Kenny, Eoin Delaney and Barry Smyth</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2021/0609.pdf">IJCAI 2021</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->








            <!-- Start One paper-->
            <h3>Generating Plausible Counterfactual Explanations for
Deep Transformers in Financial Text Classification</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="assets/coling2020.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show how to generate counterfactual explanations for NLP in a model agnostic way.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> Corporate mergers and acquisitions (M&A) account for billions of dollars of investment globally every year, and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust and accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.






                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Linyi Yang, Eoin M Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, Ruihai Dong</i>
          </p>
          <p>
                [<a href="https://aclanthology.org/2020.coling-main.541/">COLING 2020</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->




















      <h3>Academic Services</h3>
      <ul>
	      <li>General Chair, NeurIPS 2023</li>
	      <li>Program Chair, NeurIPS 2022</li>
	      <li>General (Local) Chair, FAccT 2022</li>
	      <li>Program Chair, ICLR 2021</li>
	      <li>Tutorials Chair, NeurIPS 2019</li>
	      <li>Diversity & Inclusion Chair, ICLR 2019</li>
	      <li>Action Editor, Transactions of the ACL</li>
        <li>Area Chair, ACL 2015, 2019, 2021</li>
	      <li>Area Chair, EMNLP 2015, 2019, 2020</li>
	      	      <li>Associate Editor, ACM Transactions on Social Computing</li>
	      <li>Track Chair, WWW 2019</li>
	 <li>Subcommittee Associate Chair, CHI 2018, 2019, 2020</li>
        <li>Program Chair, ICWSM 2014</li>
		<li>Senior PC, PC, Reviewer for ACL, EMNLP, NAACL, EACL, NeurIPS, ICLR, WSDM, KDD, WWW, CHI, CSCW, AAAI</li>
      </ul>








    </section>
  </div>
</body>
</html>
