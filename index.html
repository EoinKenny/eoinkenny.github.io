<!doctype html>
<html>
  <head>
    <!-- Page setup -->
    <meta charset="utf-8">
    <title>Portfolio Website</title>
    <meta name="description" content="Eoin Kenny: Researcher Personal Website">
    <meta name="author" content="Postoctoral Associate at MIT">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
    <link rel="icon" type="image/png" href="favicon.png">
  
    <!-- Stylesheets -->
    <!-- Reset default styles and add support for google fonts -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" rel="stylesheet" type="text/css" />
    <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css" />
   
    <!-- Custom styles -->
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>    

    <!-- Want to add Bootstrap? -->
    <!-- Visit: https://getbootstrap.com/docs/4.3/getting-started/introduction/ -->
    
  </head>
  
  <body>

    <header id="header">
      <img src="logo.jpg", width="200">
      <h1>Eoin M. Kenny - Postdoc at MIT</h1>
      
      <!-- Menu link fragment #id should match a div id. Example: <a href="#home"> links to <div id="home"></div>  -->
      <ul class="main-menu">
        <li><a href="#home">Home</a></li>
        <li><a href="#research">Research</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>                 
    </header>
   
    <div id="container">
      <div class="inner">
        <div id="content"> 
          
          <div id="home" class="content-region hide">
            <h2>Home</h2>

            <p>
              I am an explainable AI reseracher. Previously, I did my Ph.D. at University College Dublin, Ireland. For my undergrad, I completed my Bachelor of Music degree (and Master of Arts in Musicology & Performance) at the University of Maynooth. Currently, I am at MIT primarily researching interpretable deep reinforcement learning for my postdoc.
            </p>

            <p>
              My research vision is to design interpretable AI systems which reason in the same way humans do, so that we can clearly see what they are doing in a causally faithful way that everyone can understand (not just ML experts). This will help us successfully utilize their abilities in sensitive domains such as medicine, finance, and autonomous vechicles where people with various backgrounds need to interact with the systems.
            </p>

            <p>
              Specifically, I believe in using exemplar/prototype theory, contrative explanation, and causal reasoning (either in the sense of the world or model) in this process. My strongest contributions to the field have been (1) the introduction of Semi-Factual explanation, and (2) designing the first inherently interpretable Deep RL system.
            </p>

            <p>
              Above all however, I am a strong advocate for testing these systems on real humans to prove their utility.

            <p>
              Feel free to reach out if you would like to talk.
            </p>

          </div>
          
          <div id="research" class="content-region hide">
            <p>






            <!-- Start One paper-->
            <h3>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/iclr2023.png" width="400">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We build the first inherently interpretable, general, well performaning, deep reinforcement learning algorithm.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an "interpretable-by-design" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.
                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Mycal Tucker, Julie A. Shah</i>
          </p>
          <p>
                [<a href="https://openreview.net/forum?id=hWwY_Jq0xsN">ICLR 2023</a>] * Spotlight Presentation (top 25% of accepted papers)
          </p>
        </div>
            <!-- End One paper-->






            </p>
          </div>
          
          <div id="contact" class="content-region hide">
            <h2>Contact</h2>
            <p>
              email: ekenny (at...) mit (dot...) edu
            </p>
            <p>
              <a href="https://www.linkedin.com/in/eoin-kenny-92b57b22/">LinkedIn</a>
            </p>
            <p>
              <a href="Twitter: https://twitter.com/EoinKNNy/">Twitter</a>
            </p>
          </div>


          
        </div>
      </div>
    </div>

<!--     <footer>  
      Footer region
    </footer>
 -->    
    <!-- Load additional JS scripts here -->
    <script type="text/javascript" src="script.js"></script>
    
  </body>
</html>
