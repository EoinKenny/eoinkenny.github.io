<!doctype html>
<html>
  <head>
    <!-- Page setup -->
    <meta charset="utf-8">
    <title>Portfolio Website</title>
    <meta name="description" content="Eoin Kenny: Researcher Personal Website">
    <meta name="author" content="Postoctoral Associate at MIT">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
    <link rel="icon" type="image/png" href="favicon.png">
  
    <!-- Stylesheets -->
    <!-- Reset default styles and add support for google fonts -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" rel="stylesheet" type="text/css" />
    <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css" />
   
    <!-- Custom styles -->
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>    

    <!-- Want to add Bootstrap? -->
    <!-- Visit: https://getbootstrap.com/docs/4.3/getting-started/introduction/ -->
    
  </head>
  
  <body>

    <header id="header">
      <img src="logo.jpg", width="200">
      <h1>Eoin M. Kenny - Postdoc at MIT</h1>
      
      <!-- Menu link fragment #id should match a div id. Example: <a href="#home"> links to <div id="home"></div>  -->
      <ul class="main-menu">
        <li><a href="#home">Home</a></li>
        <li><a href="#research">Research</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>                 
    </header>
   
    <div id="container">
      <div class="inner">
        <div id="content"> 
          
          <div id="home" class="content-region hide">
            <h2>Home</h2>

            <p>
              I am an explainable AI reseracher. Previously, I did my Ph.D. at University College Dublin, Ireland. For my undergrad, I completed my Bachelor of Music degree (and Master of Arts in Musicology & Performance) at the University of Maynooth. Currently, I am at MIT primarily researching interpretable deep reinforcement learning for my postdoc.
            </p>

            <p>
              My research vision is to design interpretable AI systems which reason in the same way humans do, so that we can clearly see what they are doing in a causally faithful way that everyone can understand (not just ML experts). This will help us successfully utilize their abilities in sensitive domains such as medicine, finance, and autonomous vechicles where people with various backgrounds need to interact with the systems.
            </p>

            <p>
              Specifically, I believe in using exemplar/prototype theory, contrative explanation, and causal reasoning (either in the sense of the world or model) in this process. My strongest contributions to the field have been (1) the introduction of Semi-Factual explanation, and (2) designing the first inherently interpretable Deep RL system.
            </p>

            <p>
              Above all however, I am a strong advocate for testing these systems on real humans to prove their utility.

            <p>
              Feel free to reach out if you would like to talk.
            </p>

          </div>
          
          <div id="research" class="content-region hide">
            <p>






            <!-- Start One paper-->
            <h3>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/iclr2023.png" width="100%">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We build the first inherently interpretable, general, well performaning, deep reinforcement learning algorithm.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an "interpretable-by-design" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.
                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny, Mycal Tucker, and Julie A. Shah</i>
          </p>
          <p>
                [<a href="https://openreview.net/forum?id=hWwY_Jq0xsN">ICLR 2023</a>] * Spotlight Presentation (top 25% of accepted papers)
          </p>
        </div>

        <hr>
            <!-- End One paper-->



            <!-- Start One paper-->
            <!--  <h3>Chapter 13 -  User tests & techniques for the post-hoc explanation of deep learning</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/explainableDeepAI.jpg" width="400">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We contexualise post-hoc explanation and show a novel method to generate semi-factual and counterfactual images.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> This chapter elaborates on some key ideas and studies designed to provide post-hoc explanations-by-example to the problem of explaining the predictions of black-box deep-learning systems (the so-called XAI problem). With a focus on image and time series data we review several recent explanation strategies – using factual, counterfactual, and semifactual explanations – for Deep Learners. Several novel evaluations of these methods are reported showing how well these methods work, along with representative outputs that are produced. The chapter also profiles the user studies being carried out on these methods, discussing the pitfalls that arise and results found.


                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin Delaney, Eoin M. Kenny, Derek Greene and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/B9780323960984000193?via%3Dihub">Explainable Deep Learning AI 2023</a>]
          </p>
        </div>

        <hr> -->
            <!-- End One paper --> 










            <!-- Start One paper-->
            <h3>Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XAI</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/kbs 2022.jpg" width="600">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We show how the twin-system framework can be used to explain tabular, image, and text data predictions with post-hoc explanation by example..

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In this paper, the twin-systems approach is reviewed, implemented, and competitively tested as a post-hoc explanation-by-example solution to the eXplainable Artificial Intelligence (XAI) problem. In twin-systems, an opaque artificial neural network (ANN) is explained by “twinning” it with a more interpretable case-based reasoning (CBR) system, by mapping the feature weights from the former to the latter. Extensive comparative tests are performed, over four experiments, to determine the optimal feature-weighting method for such twin-systems. Twin-systems for traditional multilayer perceptron (MLP) networks (MLP–CBR twins), convolutional neural networks (CNNs; CNN–CBR twins), and transformers for NLP (BERT–CBR twins) are examined. In addition, Feature Activation Maps (FAMs) are explored to enhance explainability by providing an additional layer of explanatory insight. The wider implications of this …



                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/S0950705121007929">Knowledge Based Systems 2022</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->






            <!-- Start One paper-->
            <h3>Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/aij2021.png" width="600">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We find that nearest neighbor exemplar-based explanation lead people to view classifiction errors as being “less incorrect”, moreover they do not improve trust.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In this paper, we describe a post-hoc explanation-by-example approach to eXplainable AI (XAI), where a black-box, deep learning system is explained by reference to a more transparent, proxy model (in this situation a case-based reasoner), based on a feature-weighting analysis of the former that is used to find explanatory cases from the latter (as one instance of the so-called Twin Systems approach). A novel method (COLE-HP) for extracting the feature-weights from black-box models is demonstrated for a convolutional neural network (CNN) applied to the MNIST dataset; in which extracted feature-weights are used to find explanatory, nearest-neighbours for test instances. Three user studies are reported examining people's judgements of right and wrong classifications made by this XAI twin-system, in the presence/absence of explanations-by-example and different error-rates (from 3-60%). The judgements gathered include item-level evaluations of both correctness and reasonableness, and system-level evaluations of trust, satisfaction, correctness, and reasonableness. Several proposals are made about the user's mental model in these tasks and how it is impacted by explanations at an item- and system-level. The wider lessons from this work for XAI and its user studies are reviewed.



                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M Kenny, Courtney Ford, Molly Quinn, Mark T Keane</i>
          </p>
          <p>
                [<a href="https://www.sciencedirect.com/science/article/pii/S0004370221000102">Artificial Intelligence 2021</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->





            <!-- Start One paper-->
            <h3>On generating plausible counterfactual and semi-factual explanations for deep learning</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/aaai2022.png" width="600">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We introduce the AI world to semi-factuals, and show a plausible way to generate them (and counterfactuals) using a framework called PIECE.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet, counterfactual explanations have become very popular in eXplainable AI (XAI) due to their asserted computational, psychological, and legal benefits. In contrast however, semi-factuals (which appear to be equally useful) have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly because the non-discrete nature of images makes good counterfactuals difficult to define; indeed, generating plausible counterfactual images which lie on the data manifold is also problematic. This paper advances a novel method for generating plausible counterfactuals and semi-factuals for black-box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all “exceptional” features in a test image to be “normal” from the perspective of the counterfactual class, to generate plausible counterfactual images. Two controlled experiments compare this method to others in the literature, showing that PIECE generates highly plausible counterfactuals (and the best semi-factuals) on several benchmark measures.




                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17377">AAAI 2022</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->






            <!-- Start One paper-->
            <h3>If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual xai techniques</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/ijcai2021.png" width="600">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We highlight the shortcomings in counterfactual XAI research evaluation, and suggest solutions.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.






                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Mark T Keane, Eoin M Kenny, Eoin Delaney and Barry Smyth</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2021/0609.pdf">IJCAI 2021</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->







            <!-- Start One paper-->
            <h3>Twin-systems to explain artificial neural networks using case-based reasoning: Comparative tests of feature-weighting methods in ANN-CBR twins for XAI</h3>

                      <div class="fh5co-spacer fh5co-spacer-xs"></div>
                      <img src="imgs/ijcai2019.png" width="600">
                      <div class='paper-title'>


                      <div class='tldr'>

                        <p>

                          <b>TL;DR:</b> We propose a new framework for post-hoc explanation-by-example called Twin-Systems, and show it is much better compared to prior work.

                        </p>

                      </div>
                        <p>

            <b>Abstract:</b> In this paper, twin-systems are described to address the eXplainable artificial intelligence (XAI) problem, where a black box model is mapped to a white box “twin” that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artificial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to find cases that explain the ANN’s outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the ”twinning” of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN’s classifications.





                        </p>

        </div>

        <div class='paper-contents'>

          <p>
            <i>Eoin M. Kenny and Mark T. Keane</i>
          </p>
          <p>
                [<a href="https://www.ijcai.org/proceedings/2019/376">IJCAI 2019</a>]
          </p>
        </div>

        <hr>
            <!-- End One paper-->










            </p>
          </div>
          
          <div id="contact" class="content-region hide">
            <h2>Contact</h2>
            <p>
              email: ekenny (at...) mit (dot...) edu
            </p>
            <p>
              <a href="https://www.linkedin.com/in/eoin-kenny-92b57b22/">LinkedIn</a>
            </p>
            <p>
              <a href="Twitter: https://twitter.com/EoinKNNy/">Twitter</a>
            </p>
          </div>


          
        </div>
      </div>
    </div>

<!--     <footer>  
      Footer region
    </footer>
 -->    
    <!-- Load additional JS scripts here -->
    <script type="text/javascript" src="script.js"></script>
    
  </body>
</html>
